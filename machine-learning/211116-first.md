- 로지스틱 회귀 분석
1) 분류 모델
: 정확도가 안맞을때? 라벨이 불균형

- 혼합 행렬
| 예측 / 실제 | P | N |
| P | TP | FP |
| N | FN | TN |

1) 정확도 : TP + TN / all
2) F1 score : 2 * PR * RE / PR + RE : 실무에서 많이 사용 : 정밀도와 재현율의 조화 평균
cf. 조화 평균 : 2AB / A+B
3) Precision 정밀도 : 예측 p 만 평가 : TP / TP + FP 
4) Recall 재현율 : 실제 P만 평가 : TP / TP + FN

- regression : cost : 오차 + 감마 |w| : 감마가 커지면 w를 줄여야 하지만
- classification : cost : 오차 + 1/c |w| : c가 작아지면 w가 작아진다 : 10보다 1이 규제가 강하다 : overfitting일 때 규제 강화를 위해 c를 낮춘다 

- 클래스가 2개이면 다중 클래스가 아니다 : 클래스가 3개부터 다중 클래스이다.
1) 클래스가 2개면 경계선 1개 
2) 클래스가 3개면 경계선 3개 즉 클래스 개수 만큼

- SVC : linear 서포트 벡터에 값 부여
cf. non linear 서포트 벡터
1) 슬랙 변수 가중치 : 오차 허용률 : cost = c 오차 + |w| : c가 줄면 오차가 커져서 w가 줄어든다

- SVM(서포트 벡터 머신) : 마진 최대화 
1) cost = C 오차 + 1/마진 : C를 올리면 오차를 허용하지 않겠다는 뜻이기에 오차가 줄어든다. 대신 마진은 작아지고, 마진의 역수 값은 커진다 : trade off 관계에 있다

2) hard support vector machine 
오차가 적어지면 과적합의 문제가 생길 수 있지만, 그래도 오차가 적어지는 방향으로 가는 게 맞다. 

3) 오차를 단 한차례도 발생시키기 않을 수 있을까? 케바케이다. 

4) hinge loss : 경계선 안쪽에 들어가야 sv에 오차가 0으로 수렴한다. 

5) cost = C 오차 + 1/마진
회귀 : 오차 = (실제값 - 예측 값)^
로지스틱 : 오차 = log loss : 오차 범위 = 0 < < 무한대
SVM : 오차 = hinge loss : 오차 범위 = 0 < < 무한대 : 예측 = 0, 1

- 소프트 마진 : C 값을 1 -> 0.1 : 오차를 많이 허용하겠다 : 마진을 최대화 (거리 개념)
- 하드 마진 : 오차를 허용하지 않는다 : 마진을 줄인다

- 데이터 스케일링 : 서포트 벡터 머신에서 사용해야 한다 
1) 단위의 문제가 아니라 크기의 문제 (mm , m)

- 문제 : x feature의 개수가 5개, class가 3개인 다중 분류라면, logistic w의 개수와 SVM w의 개수는 무엇인가? 
- 팁 :  bias 포함해서 계산, 경계선, 각 경계선에 필요한 w 수
1) 로지스틱 regression 
경계 3개(경계선이 같을 수 없음)
경계선의 방정식 : W1X1 + W2X2 + W3X3 + W4X4 + W5X5 + bias = 0
weight의 개수는 (bias 포함) 18개 (제외하면) 15개
2) SVM 
경계 3개
weight 개수 (bias 포함) 18개, (제외하면) 15개

-> 둘 다 weight의 개수는 같으나, 다르게 학습되어서 값이 다를 것이다. 

- iris 붓꽃 데이터로 실습하기 

- SVM perceptron 퍼셉트론
1) 선형 분류 말고 비선형 데이터  분류는 어떻게? 
2) hypothesis : 0 (Wx <= 0) or 1 (Wx >= 0) : Wx를 선형에서 비선형으로 분류
3) SVM -> Kernel : X1^ + X2^ = r^ : a + b = r^ : b = -a + r^ 
4) rbf, poly, sigmoid
5) 여러 층을 쌓아서 비선형 데이터 문제를 해결한다

- 다층 퍼셉트론
1) 커널은 사람이 직접
2) 자유도를 높게 사람의 개입을 빼고 기계의 자유도를 주는 형식으로
3) A 공간 : 단일 레이어 > 선형 분류
4) X를 A 공간으로 바꿔서 선형 분류시키겠다 = X 공간에서 비선형 분류를 하겠다

- 딥러닝의 핵심 개념
0) 학습 방법을 알 수 없는 블랙 박스는 맞지만, 동작 원리를 알 수 있다
1) 딥러닝 학습 방법 2단계 -  (1) 비선형 x -> 선형 a : 공식 찾기  (2) 단일 퍼셉트론 선형 경계선 
2) 아직도 비선형이 선형이 되지 않았다면 마지막 단계에서 해결 할 수 없다 : 마지막 단계 직전에 해결이 되어야 한다. 비선형이 풀려야 한다
3) 비선형성을 풀어주는 방법 : a와 b를 X1과 X2의 어떤 식으로 만들 예정인데, X1과 X2로 만들어낸 식이 비선형적이어야 한다. 

- Decision Tree
1) 또 하나의 비선형 모형
2) 결정나무는 마치 스무고개와 같다 
스무고개와 다른점 -> 정답이 되는 후보 적어서 정해져 있음, 질문을 던질 수 있는 바운더리가 있다.
3) 정보 획득(Information Gain)
4) Entropy = 불순도 : 데이터 집단에 서로 다른 카테고리가 골고루 뒤섞여있으면 불순도는 높다고 본다 : 하나의 카테고리만 존재하는 경우를 불순도가 없는 순도 100퍼센트 데이터 집단
