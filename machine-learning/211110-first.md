1. 머신러닝이 뭐야?
: 기계가 사람처럼 행동하도록 학습시키는 것(자율주행)
: 인공지능 > 머신러닝 > 딥러닝

- 인공지능 : 규칙 기반, 학습 기반(=머신러닝)

- 머신러닝 : 규칙 기반 없이, 데이터(혹은 추억) : From DATA Maxhine Learning : 단 데이터는 많아야 한다 = 빅데이터

- 컴퓨터가 데이터로부터 학습을 한다? 
1) 데이터 상관관계(Hypothesis) : 상관관계를 잘 찾는 것이 핵심 
2) Hypothesis는 머신러닝의 목적이고, cost는 Hypothesis의 평가 지표이다 : 비용이 낮아야 효율이 좋음
3) cost : prediction - real value 
4) 목적 : COST가 낮은 Hypothesis 찾는 것이다 
5) 여기서 학습하는 것은 weight (= 10x)
6) Hypothesis : y = 10x or y = x^3 - 6x^ + 21x - 6
7) hypothesis의 폼을 결정하고(사람) weight를 결정하는 것 (기게)
8) 머신러닝은 COST가 낮을 때의 weights를 찾는 것
*COST가 낮으면 과적합 문제가 있을 수 있기에 규제가 필요함

- 머신러닝 : 지도학습(Supervised Learning) vs 비지도학습(Unsupervised Learning)
1) 지도 학습 : x와 y의 관계 파악 : 새로운 x의 결과 '예측'
2) 비지도 학습 : x끼리의 관계 파악 : x의 군집화 혹은 변환
-> 예측이 더 쉽고 많음
3) 데이터 = 목적 : 데이터의 형태를 보면 학습의 목적을 알 수 있다

- 지도 학습도 다 같은 지도 학습이 아니다?
1) 회귀분석 Regression : 숫자를 예측
2) 분류 Classification : 숫자로 라벨링된 카테고리를 예측

- 비지도 학습
1) 군집화 : x간의 관계 파악 -> 그룹화
2) 변환 : x간의 관계 파악 -> 특성 추출

2. 컴퓨터가 어떻게 학습을 하지?
- 부동산 데이터 : 사이즈와 매매가의 관계
- 직선 == size와 price 관계 = x와 y의 관계 = Hypothesis
1) weight 초기값 설정
2) cost 계산 : 예측값 - 실제값인 err의 평균 값
3) cost가 낮아지는 방향으로 직선 업데이트 : 미분으로 알 수 있다
4) cost가 가장 낮을 떄의 가중치를 어떻게 찾느냐 : 가장 낮을 때의 가중치를 어떻게 찾느냐 : 기울기가 완만해지는 방향 (기울기란 x가 1만큼 증가하면 y가 2만큼 증가한다 : y = 2x )
5) cost가 낮아지는 방향은 기울기를 뺀다는 것이다 : Weight - Gradient : 기울기가 양수면 w를 감소하고, 기울기가 음수면 w를 증가 
6) y(cost) = x^(weight), G = y/x : 변화량

머신러닝 = 데이터를 통해 머신이 '데이터관계(Weights)'를 학습하는 것
How? 
1) weight 초기값을 설정한 다음 
2) 그 순간의 Cost 계산하고
3) 그 순간의 Cost 함수의 기울기를 계산해서
Wt+1 <- Wt - @*G

머신러닝 = Cost가 가장 낮을 때의 Weights를 찾는 것이다. -> 수식으로 표현
A가 가장 작을 때의 B를 찾는다 = 수식으로 표현하자면? argument min

- 나에게 데이터셋 10만 개가 있다면?
1) 일부 학습 / 일부 검증 = 9:1 ~ 5:5
2) 검증 결과의 신뢰도를 높이기 위해 교차검증을 수행한다
3) 학습을 잘한다는 의미는 ? 학습 문제도 잘 맞추지만 학습 문제의 전체적인 경향도 잘 파악해서, 같은 경향의 새로운 문제가 나와도 잘 맞추는 것(일반화) 
-> 규제화를 통해 일반화를 달성 
-> 모델의 복잡도에 제한 
-> 수치 찾기

- 학습문제 전체적인 경향도 잘 파악하면서 학습하게 하려면 ? 
1) 문제의 복잡도에 적합하게 학습해야 한다
2) 경계선의 복잡도 = 수치화 = 너무 단순하지도 복잡하지도 않게 적절한 수치로 머물 수 있도록 weight 학습 

- 데이터와 모델의 복잡도가 서로 잘 맞는 것이 중요 
1) 데이터를 모델에 잘 맞게 변형 : 정규화, 차원 축소
2) 모델을 데이터에 잘 맞게 변형 : 규제화 

COST = 실제 - 예측 
Weight를 찾아라

- 선형회귀분석모형(Linear Regression Hypothesis)
1) Hypothesis를 어떤 형태로 찾을지 우리가 설정해주는 작업
2) 최소제곱법 : Wx + b : 비교적 단순하다 : 선형적이다 : inf, -inf
3) 컴퓨터야 x와 y의 관계를 y = Wx + b의 형태로 찾을거야. 거기서 W랑 b만 너가 찾아줘.
4) cost : 데이터의 크기 개수를 예측값 - 실제값

- Linear Regression 의 규제화
1) 방정식보다 미지수가 많은 경우 해는 무수히 많다 : 방정식의 수 = 데이터의 수 : 미지수의 수 = weight의 수 
2) 데이터 보다 weight가 많은 경우 해는 무수히 많다 = overfitting이 일어날 수 있다
3) L1, L2 norm : weight의 크기에 규제를 건다
4) weight의 크기는 개수에 비례한다

모델의 복잡도가 크면 과적합 가능성이 커진다
모델의 복잡도 == w크기 == w개수
모델의 가설함수를 봤더니 , w 개수가 많네? 
-> 복잡도 높은 거 아냐?
-> 과적합 나오기 쉬운 모델 아냐?
-> 규제 걸어야 되는 거 아냐?

규제를 어떻게 건다? 
-> w크기를 줄이면서
cf. 모델의 복잡도를 제한하는 규제를 통해 학습 모델 일반화

규제 : 모델의 복잡도에 제한 : 데이터는 그대로 두고 모델을 데이터에 맞추는 것. W개수 조절하는 것은 어패가 있다. (개수가 아닌 크기)
-> 중요한 X 피처를 선별하겠다 -> 데이터를 변형 : 피처 엔지니어링 영역이다.

- Ridge Regression
1) Linear Regression + 규제화 : Ridge Regression : cost 변형을 주어야 함
2) cost 함수 : 오차(예측 - 실제) + L2 norm(w^ 합친 것)
3) L2 norm : 규제항

- Ridge Regression cost
1) 규제가 크다 = weight의 크기가 조금만 커져도 cost에 영향이 크기 때문에 weight를 상대적으로 작게 만들어야 한다
2) 규제의 강도의 많이 줄수록 weight의 크기가 작아진다 
3) cost를 결정할 때 오차보다는 weight의 비중

- Lasso Regression
1)  Linear Regression + 규제화(L1 norm)

- Elastic Net = Ridge + Lasso
1) Elastic Net의 Cost

선형회귀 모형 

Linear, Ridge, Lasso, Elastic Net
공통 : hypothesis : Wx + b
차이 : Cost : 오차항, L2/W^, L1/|W|, 둘 다

-> Ridge와 Lasso의 차이?
