1. 변수 분포 문제
변수 분포 문제란 일반화된 모델을 학습하는데 어려움이 있는 분포를 가지는 변수가 있어서 일반화된 모델을 학습하지 못하는 문제 
특징과 라벨 간 약한 관계 및 비선형 관계
이상치 포함
특징 간 상관성
일반 분포
변수 간 스케일 차이
특징과 라벨 간 약한 관계 및 비선형 관계
문제 정의
특징과 라벨 간 관계가 없거나 약하면 어떤 전처리 및 모델링을 하더라도 예측력이 높은 모델을 학습할 수 없음
특징과 라벨 간 비선형 관계가 존재한다면, 적절한 전처리를 통해 모델 성능을 크게 향상시킬 수 있음
대다수의 머신러닝 모델은 선형식을 포함함 : wx + b
해결 방안
특징과 라벨 간 관계를 나타내는 그래프를 통해 적절한 특징 변환을 수행해야 함
하지만 특징 개수가 많고 다른 특징에 의한 영향도 존재하는 등 그래프를 통해 적절한 변환 방법을 선택하는 것이 쉽지 않아서 다양한 변환 방법을 사용하여 특징 선택을 수행해야 함
이상치 제거
문제 정의 및 해결 방안
변수 범위에서 많이 벗어난 아주 작은 값이나 아주 큰 값으로, 일반화된 모델을 생성하는데 악영향을 끼치는 값으로 이상치를 포함하는 레코드를 제거하는 방법으로 이상치를 제거함 (절대 추정의 대상이 아님에 주의)
이상치 판단 방법 : IQR(Q3 - Q1) 규칙 활용
변수별 IQR 규칙을 만족하지 않은 샘플을 판단하여 삭제하는 방법
직관적 사용과 간편함
단일 변수로 이상치를 판단하기 어려운 경우에는 문제가 있음
numpy.quantile
이상치 판단 방법 : 밀도 기반 군집화 활용
DBSCAN 등의 밀도 기반 군집화 기법은 군집에 속하지 않은 샘플을 이상치라고 간주하므로, 밀도 기반 군집화 결과를 활용하여 이상치를 판단할 수 있음
DBSCAN 등의 밀도 기반 군집화 모델의 파라미터 튜닝이 쉽지 않다는 단점
sklearn.cluster.DBSCAN : 군집화를 수행하는 인스턴스를 생성하는 함수
특징 간 상관성 제거
문제 정의
회귀 모델, 신경망, SVM과 같이 wx + b 형태의 선형식이 모델에 포함되는 경우, 특징 간 상관성이 높으면 강건한 파라미터 추정이 어려움
트리 계열의 모델은 사실 특징 간 상관성이 높다고 해서 모델 예측 성능에 영향을 받지 않지만, 상관성이 높은 변수 중 소수만 모델에 포함되기 때문에 설명력에 크게 영향을 받을 수 있음
해결 방법 : VIF 활용
Variance inflation factors는 한 특징을 라벨로 간주하고, 해당 라벨을 예측하는데 다른 특징을 사용한 회귀 모델이 높은 결정 계수를 보이는 경우 해당 특징이 다른 특징과 상관성이 있다고 판단함
VIF가 높은 순서대로 특징을 제거하거나, VIF가 10 이상인 경우 주로 삭제함
해결 방법 : 주성분 분석
특징이 서로 직교하도록 만들어 특징 간 상관성을 줄이는 방법
n차원의 데이터는 총 n개의 주성분이 존재하지만, 차원 축소 등을 위해 분산의 대부분을 설명하는 주성분만 사용
sklearn.decomposition.PCA : 주성분 분석을 수행하는 인스턴스를 생성하는 함수
변수 치우침 제거
문제 정의
모델링에 가장 적합한 확률분포는 정규분포이나, 실제로 많은 변수가 특정 방향으로 치우쳐 있음
한 쪽으로 치우친 변수에서 치우친 반대 방향의 값 (꼬리 부분)들이 이상치처럼 작용할 수 있으므로, 이러한 치우침을 제거해야 함
탐색 방법 : 왜도(skewness)
변수 치우침을 확인하기 가장 적절한 척도로는 왜도가 있음
왜도는 분포의 비대칭도를 나타내는 통계량
왜도의 절대값이 1.5 이상이면 치우쳤다고 판단함
scipy.stats
해결 방안
변수 치우침을 해결하는 기본 아이디어는 값 간 차이를 줄이는데 있음
스케일링
문제 정의
스케일이 달라서 발생하는 문제로, 스케일이 큰 변수에 의해 작은 변수에 의해 모델이 크게 영향을 받는 문제
스케일이 큰 변수 : k-최근접 이웃
스케일이 작은 변수 : 회귀 모델, 서포트 벡터 머신, 신경망
스케일에 영향을 받지 않는 모델 : 나이브 베이즈, 의사결정나무
해결 방법
스케일링을 사용하여 변수 간 스케일 차이를 줄이는 방법
모델에 따른 스케일러 선택 : Standard Scaler(특징의 정규분포), Min-Max Scaler(특정 분포를 가정하지 않음)
sklearn.preprocessing.MinMaxScaler & StandardScaler
단 스케일의 문제가 아니라 변수 자체가 쓸모 없는 경우에는 삭제 처리 해줘야 함(판단이 중요!)

2. 클래스 불균형 문제
문제 정의 및 탐색 방법
클래스 변수가 하나의 값에 치우친 데이터로 학습한 분류 모델이 치우친 클래스에 대해 편향되는 문제
모델은 대부분 샘플을 치우친 클래시 값으로만 분류하게 됨
클래스 불균형 문제가 있는 모델은 정확도가 높고, 재현율이 매우 낮은 경향이 있음
용어 정의
다수 클래스 : 대부분의 샘플이 속한 클래스
소수 클래스 : 대부분의 샘플이 속하지 않은 클래스
위양성 비용(TP) : 부정 클래스 -> 긍정 클래스 샘플(정상인 -> 암환자 : 돈, 시간)
위음성 비용(TN) : 긍정 클래스 -> 부정 클래스 샘플(암환자 -> 정상인 : 생명)
보통은 위음성 비용이 위양성 비용보다 훨씬 큼
절대 부족 : 소수 클래스에 속한 샘플 개수가 절대적으로 부족한 상황
발생 원인
대부분의 분류 모형의 학습 목적식은 정확도를 최대화하는 것으로, 대부분 샘플을 다수 클래스라고 분류하도록 학습됨
탐색 방법 : 클래스 불균형 비율
클래스 불균형 비율 = 다수 클래스 / 소수 클래스
클래스 불균형 비율이 9 이상이면 편향된 모델이 학습될 가능성이 있음
다만 클래스 불균형 비율이 높다고 해서 반드시 편향된 모델을 학습하는 것은 아님
탐색 방법 : K-최근접 이웃을 활용하는 방법
이웃의 클래스 정보를 바탕으로 분류를 하기에 클래스 불균형에 매우 민감하므로, 클래스 불균형 문제를 진단하는데 적절함
k값이 크면 클수록 더욱 민감하므로, 보통 5~11 정도의 k를 설정하여 문제를 진단함
문제 해결의 기본 아이디어
소수 클래스에 대한 결정 공간을 넓히는 것임

재샘플링
오버 샘플링 : 소수 클래스 샘플 생성 : 원본 데이터가 작을 때 유용
언더 샘플링 : 다수 클래스 샘플 삭제 : 원본 데이터가 클 때 유용
결정 경계에 가까운 다수 클래스 샘플을 제거하고, 결정 경계에 가까운 소수 클래스 샘플을 생성해야 함
평가 데이터에 대해 절대로 재샘플링을 적용하면 안 됨

대표적인 오버샘플링 알고리즘 : SMOTE
소수 클래스 샘플을 임의로 선택하고, 선택된 샘플의 이웃 가운데 하나의 샘플을 또 임의로 선택하여 그 중간에 샘플을 생성하는 과정을 반복하는 방법
imblearn.over_sampling.SMOTE
sampling_strategy : 입력하지 않으면 1:1 비율이 맞을 때까지 샘플을 생성하며, 사전 형태로 입력하여 클래스 별로 생성하는 샘플 개수 조절 가능
k_neighbors : 이웃 수로 보통 1, 3, 5 정도로 작게 설정
.fit_sample(X, Y) : X와 Y에 대해 SMOTE를 적용한 결과를 ndarray 형태로 반환

대표적인 언더샘플링 알고리즘 : NearMiss
가장 가까운 n개의 소수 클래스 샘플까지 평균 거리가 짧은 다수 클래스 샘플을 순서대로 제거하는방법
a + b < c + d : 1번 샘플을 2번 샘플보다 먼저 삭제
sampling_strategy : 입력하지 않으면 1:1 비율이 맞을 때까지 샘플을 생성하며, 사전 형태로 입력하여 클래스 별로 생성하는 샘플 개수 조절 가능
n_neighbors : 평균 거리를 구하는 소수 클래스 샘플 수
version : NearMiss의 version으로 2를 설정하면 모든 소수 클래스 샘플까지의 평균 거리를 사용
.fit_sample(X, Y) : X와 Y에 대해 NearMiss를 적용한 결과를 ndarray 형태로 반환

비용 민감 모델
정의 : 학습 목적식에 위음성 비용(긍정 -> 부정 클래스 오분류)과 위양성 비용(부정 -> 긍정 클래스 오분류)을 다르게 설정하는 모델로, 보통 위음성 비용을 위양성 비용보다 크게 설정
위음성 비용 = w * 위양성 비용 ( w > 1)
확률 모델
로지스틱 회귀, 나이브 베이즈의 확률 모델을 cut off value, c를 조정하는 방식으로 비용 민감 모델 구현
정확한 확률 추정 불가하지만 그 개념을 도입할 수 있는 모델(K-최근접 이웃, 신경망, 의사결정나무, 앙상블 모델 등)에도 적용 가능함
predict_proba
Numpy와 Pandas를 잘 쓰는 기본 원칙 : 가능하면 배열 단위 연산하기 : 유니버설 함수, 브로드캐스팅, 마스크 연산 활용
비 확률 모델
서포트 벡터 머신
의사결정나무 : 소수 클래스에 가중치를 부여하는 방식으로 가능하면 소수 클래스로 분류하도록 유도
 class_weight

3. 차원의 저주 문제
문제 정의
차원이 증가함에 따라 필요한 데이터의 양과 시간 복잡도가 기하급수적으로 증가하는 문제
차원을 줄여야 하는 이유
차원이 증가함에 따라 모델 학습 시간이 정비례하게 증가
차원이 증가하면 각 결정 공간에 포함되는 샘플 수가 적어져서 과적합으로 이어지고 성능 저하가 발생함
특징 선택
개요 : 분류 및 예측에 효과적인 특징만 선택하여 차원을 축소하는 방법
적용 대상 : 특징 선택은 특징이 많은 데이터에만 적용하지 않는다!
주먹구구식 특징 선택
선택 가능한 모든 특징 집합을 비교/평가하여 가장 좋은 특징 집합을 선택
그러나 특징 개수가 n개라면, 2n-1번의 모형 학습이 필요하므로, 현실적으로 적용 불가함.
필터링 기반의 특징 선택
특징과 라벨이 얼마나 관련이 있는지를 나타내는 클래스 관련성이 높은 특징을 우선 선택하는 방법
클래스 관련성
한 특징이 클래스를 얼마나 잘 설명하는지를 나타내는 척도로, 상관계수, 카이제곱 통계량, 상호 정보량 등의 특징과 라벨 간 독립성을 나타내는 통계량을 사용하여 측정
즉 클래스 관련성이 높은 특징은 분류 및 예측에 도움이 되는 특징이며, 그렇지 않은 특징은 도움이 되지 않은 특징
F-통계량 : 집단 간 평균 차이 측정
클래스 관련성 척도 분류
카이제곱 통계량, 상호정보량, F-통계량
sklearn.feature_selection.SelectKBest

4. 실전 모델 개발 프로세스
큰 그림
실제 프로세스에서는 탐색과 전처리 사이에 피드백 루프 존재함
탐색을 한 번에 다하고 전처리를 한 번에 다하는 것이 아님
전처리에도 다양한 하이퍼 파라미터가 있기에 모델의 하이퍼 파라미터와 같이 튜닝해야 함
필수 전처리
특별한 튜닝이 필요하지 않으므로 순차 진행
데이터 파편화 여부 확인 > 데이터 통합
결측 여부 확인 > 결측치 제거/대체/예측
범주형 변수 확인 > 범주형 변수 처리(더미화, 연속형 변수 처리)
라벨 문자 여부 확인 > 라벨 변환
성능 향상을 위한 전처리
특징과 라벨 간 상관관계 확인 > 신규 특징 추가
이상치 확인 > 이상치 제거
특징 간 상관관계 확인 > 특징 간 상관성 제거
왜도 확인 > 변수 치우침 제거
특징 간 스케일 차이 확인 > 스케일링
클래스 불균형 확인 > 재샘플링 및 비용 민감 모델 고려
데이터 크기 확인 > 특징 선택 & 모델 목록 정의
파라미터 그리드 설계
탐색을 제대로 하지 않으면 파라미터 그리드에 포함된 파라미터 개수가 수십 ~ 수백만개도 됨
이상치 제거 파라미터 > 재샘플링 모델 및 파라미터 > 특징 선택 파라미터 > 모델 및 파라미터

