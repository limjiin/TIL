- 디시젼트리 예측방식(Hypothesis)

- 디시젼트리 학습방식(Cost)
1) 타이타닉 1000 : train 800, test 200 : 무엇을 학습하는 건가?
2) X Y 상관관계 > cost가 최저점일 때 weight > entropy(불순도)가 낮은 질문을 찾는 것이다
3) Greedy하게 찾는다 : 던질 수 있는 질문을 다 던져 본다 : entropy가 가장 낮은 질문을 픽한다.
4) recursive partitioning 반복적 분할 > 언제 멈출까? > 순도 100%에 도달할 때 더이상 쪼개지지 않는다.
5) train 할 때 모든 잎부분이 불순도 0이 되면 항상 좋은 걸까? 과적합이 일어날 가능성이 매우 높다
6) root mode > leaf : cost가 낮아지는 w의 값 : 최적화 optimizinf 
7) cost = 오차 + 규제 |w| (모델복잡도(질문의 깊이, 개수) 낮음)
8) 질문의 깊이가 낮으면 모델복잡도가 단순하다. 반면, 질문의 깊이가 너무 깊으면 또는 질문의 개수가 너무 많으면 모델복잡도가 복잡해져서 과적합이 일어날 수 있다 

- 총정리
1) 좋은 질문은 entropy가 낮은 것
2) 질문 한 번에 x feature 하나만 골라서 질문
3) 따라서 수평 또는 수직만 가능하고 대각선으로 그릴 수 없다 : 단순하게 질문하겠다는 뜻
4) 골고루 : entropy(불순도) 높은 것

- 추가
1) decision tree는 머신러닝이 이것을 왜 선택했는지 설명이 가능하다
2) 분류 (학습 : entropy, 예측 : 최빈값)> entropy> label > 비중 > 최빈값 > 예측
3) 회귀 : entropy를 얘기할 수 없음 왜냐면 라벨(label)이 연속적이기 때문에  > 어떻게 질문을 판별하냐? 평균값과 그것과 떨어진 거리 : 분산 : partitioning 후 비슷한 애들끼리 나눈 후 평균값을 구해서 거리를 구하기 때문에 분산이 더 작다.
4) DTR 
.학습 : 분산(=오차)을 낮추는 질문
.예측 
.단 범위를 벗어날 수 없음 

- 디시젼트리
1) 혼자보다 함께 : 하나의 모형만을 독립적으로 사용하기 보다는 서로 다른 디시젼트리 여러 개를 함께 사용한다. 하나로는 학습이 잘 되지 않기 때문이다.
2) 앙상블 : 하나의 모형으로 학습이 잘 안되어서 여러개 사용하는 것


- 랜덤 포레스트 : 어떤 무작위성을 가지고 있을까 ?
1)  서로 다른 트리 : 무작위성 

- 앙상블 : 배깅 
1) Bootstrap(원본 데이터 집단에서 원본과 유사한 새로운 데이터 집단을 추출) aggregation(모으다 합치다)
: 즉 여러 개의 원본과 유사한 데이터를 만들어서 포레스트를 만들기 위해
: 원본에서 랜덤 추출한다.
2) soft vote : 확률의 평균값
3) hard vote : ox의 비율
-> 일반적으로 soft vote를 사용한다

- 랜덤 포레스트의 한계 
1) 단순 aggregating을 하다보니 depth 측면에서 늘어나지 않는다. 
(독립 > voting : 병렬적으로 합쳐서 의견을 모으는 것 : 모은 의견을 판별하는 방법 soft(확률의 평균)/ hard(의견의 비율) )
2) 모델 복잡도를 높일 수 있는 방법이 없는 단순한 모델. 초창기 앙상블모형이기 때문에. 
3) Decision Tree의 일반화(오차 최소화 아님)를 늘린 것이 Random Forest
4) Decision Tree는 recursive partitioning 때문에 쉽게 overfitting에 빠짐 
5) 오차 최소화를 고도화 시키기 못했기에(오차최소화에 취약해서 모델복잡도가 높아지지 않음), 이를 해결하기 위해 boosting 방법인 Ada Boosting를 만들었다. -> 학습이 잘 되지 않은 underfit 문제가 생김

- 앙상블 : Boosting
1) 먼저 학습하고 그 다음 학습 그다음 학습
cf. 배깅 : 한꺼번에 학습하고 모으는 것
2) Ada > GBM > XG > Light XG
 
