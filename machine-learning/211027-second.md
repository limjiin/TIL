Part.03 기본적인 머신러닝 모형
1. 나이브 베이즈 : Naive bayes classifier 배경
특정, 겉보기 날씨와 습도가 고정되어 있다고 할 때, 가질 수있는 패턴은 두 가지
위 패턴이 많을 때, 테니스를 칠 확률이 높다.
위 패턴이 많으려면? 
설명변수와 반응변수를 분리하여 생각
아래와 같은 상황일 때, 해당 패턴이 자주 나타난다
테니스 자체를 많이 치는 경우
테니스를 쳤을 때, 해당 겉보기 날씨와 습도가 자주 출현한 경우
나이브 베이즈는 설명 변수 간의 독립을 가정
테니스를 쳤을 때, 겉보기 날씨와 습도는 서로 상관관계가 없음을 가정

2. 나이브 베이즈 : Naive bayes classifier 수학적 개념 이해 : 조건부 확률, 베이즈 정리
수학적 개념 이해 : 조건부 확률
특정, 겉보기 날씨와 습도가 고정되어 있다고 할 때, 테니스를 칠 확률이 높으려면?
특정, 겉보기 날씨와 습도가 주어졌을 때, 테니스를 칠 조건부 확률이 높아야 한다.
수학적 개념 이해 : 베이즈 정리
위 패턴이 많으려면?
설명변수와 반응변수를 분리하여 생각
아래와 같은 상황일 때, 해당 패턴이 자주 나타난다.
테니스 자체를 많이 치는 경우
테니스를 쳤을 때, 해당 날씨와 습도가 자주 출현한 경우

3. 나이브 베이즈 : Naive bayes classifier 종류 및 이해
Gaussian Naive bayes classifier : 설명 변수가 연속형인 경우
Multinormal Naive bayes classifier : 설명 변수가 범주형인 경우

3-1. 나이브 베이즈 실습

### 1. Gaussian Naive Bayes

- 데이터, 모듈 불러오기

from sklearn import datasets
from sklearn.naive_bayes import GaussianNB
import pandas as pd

iris = datasets.load_iris()
df_X=pd.DataFrame(iris.data)
df_Y=pd.DataFrame(iris.target)

df_X.head()
df_Y.head()

- 모델 피팅

gnb = GaussianNB()
fitted = gnb.fit(iris.data, iris.target)
y_pred = fitted.predict(iris.data)

fitted.predict_proba(iris.data)[[1, 48, 51, 100]]

fitted.predict(iris.data)[[1, 48, 51, 100]]

- Confusion matrix 구하기

from sklearn.metrics import confusion_matrix

confusion_matrix(iris.target,y_pred)

- Prior 설정하기

gnb2 = GaussianNB(priors = [1/100, 1/100, 98/100])
fitted2 = gnb2.fit(iris.data, iris.target)
y_pred2 = fitted2.predict(iris.data)
confusion_matrix(iris.target, y_pred2)

gnb2 = GaussianNB(priors = [1/100, 98/100, 1/100])
fitted2 = gnb2.fit(iris.data, iris.target)
y_pred2 = fitted2.predict(iris.data)
confusion_matrix(iris.target, y_pred2)

### 2. Multinomial naive bayes

- 모듈 불러오기 및 데이터 생성

from sklearn.naive_bayes import MultinomialNB
import numpy as np

X = np.random.randint(5, size=(6, 100))
y = np.array([1, 2, 3, 4, 5, 6])

- Multinomial naive bayes 모델 생성

clf = MultinomialNB()
clf.fit(X, y)

print(clf.predict(X[2:3]))

clf.predict_proba(X[2:3])

- prior 변경해보기

clf2 = MultinomialNB(class_prior = [0.1, 0.1999, 0.0001, 0.1, 0.1, 0.1])
clf2.fit(X, y)

clf2.predict_proba(X[2:3])

4. KNN (K-Nearest neighborhood) 배경
K는 어떻게 정하는 가?
너무 큰 K : 미세한 경계 부분을 잘못 분류할 것
너무 작은 K : 이상치의 영향을 크게 받을 것, 패턴이 직관적이지 않을 것
중요한 변수와 불필요한 변수가 섞여 있다면? 중요한 변수를 선별할 필요가 있음
종속 변수
범주형 변수
KNN 중 가장 많이 나타나는 범주로 y를 추정
Tie 문제를 막기 위해 K는 홀수로 정하는 것이 좋다
연속형 변수
KNN의 대표값(평균)으로 y를 추청
거리는 어떻게 구하나?
설명 변수
범주형 변수 : Hamming distance
연속형 변수 : 유클리안 거리, 맨하탄 거리

5. KNN (K-Nearest neighborhood) 모델 및 정의, Cross validation
과적합의 문제
Training set을 가장 잘 맞히는 머신은 Test set에서는 잘 동작하지 않을 수 있다
Training error는 error를 과소추정하는 성향이 있음

6. KNN 정리
K의 결정
너무 큰 K : 미세한 경계 부분을 잘못 분류할 것
너무 작은 K : 이상치의 영향을 크게 받을 것, 패턴이 직관적이지 않을 것
Training error : k = 1에서 가장 낮음, 과적합의 가능성
Test error : 데이터에 따라 최적의 k가 존재
K의 결정 : Test error를 작게하는 k, cross-validation 이용
차원의 저주
Y축 정보가 추가됨에 따른 비효율
차원 축소 : Principal Component Analysis, Canonical Correlation Analysis 등의 방법을 활용하여 차원을 축소한 후 KNN 진행

6-1. KNN 실습

### 1. 데이터, 모듈 불러오기 및 kNN 피팅 방법

- 함수 불러오기

from sklearn import neighbors, datasets
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.metrics import confusion_matrix

iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

- 모델 구축

clf = neighbors.KNeighborsClassifier(5)
clf.fit(X, y)

y_pred=clf.predict(X)

confusion_matrix(y,y_pred)

### 2.Cross-validation을 활용한 최적의 k찾기

- 함수 불러오기
- "from sklearn.cross_validation import cross_val_score" 코드가 아래와 같이 변경되었습니다.

from sklearn.model_selection import cross_val_score

- CV 진행

k_range = range(1, 100)
k_scores = []

for k in k_range:
    knn = neighbors.KNeighborsClassifier(k)
    scores = cross_val_score(knn, X, y, cv = 10, scoring='accuracy')
    k_scores.append(scores.mean())

plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-validated accuracy')
plt.show()

### 2.Weight를 준 kNN

n_neighbors = 40

h = .02  # step size in the mesh

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
[O    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()

np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()
y[::5] += 1 * (0.5 - np.random.rand(8))

knn = neighbors.KNeighborsRegressor(n_neighbors)
y_ = knn.fit(X, y).predict(T)

n_neighbors = 5

for i, weights in enumerate(['uniform', 'distance']):
    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)
    y_ = knn.fit(X, y).predict(T)

    plt.subplot(2, 1, i + 1)
    plt.scatter(X, y, c='k', label='data')
    plt.plot(T, y_, c='g', label='prediction')
    plt.axis('tight')
    plt.legend()
    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,
                                                                weights))

plt.tight_layout()
plt.show()
