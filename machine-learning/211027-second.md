Part.03 ê¸°ë³¸ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨í˜•
1. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ : Naive bayes classifier ë°°ê²½
íŠ¹ì •, ê²‰ë³´ê¸° ë‚ ì”¨ì™€ ìŠµë„ê°€ ê³ ì •ë˜ì–´ ìˆë‹¤ê³  í•  ë•Œ, ê°€ì§ˆ ìˆ˜ìˆëŠ” íŒ¨í„´ì€ ë‘ ê°€ì§€
ìœ„ íŒ¨í„´ì´ ë§ì„ ë•Œ, í…Œë‹ˆìŠ¤ë¥¼ ì¹  í™•ë¥ ì´ ë†’ë‹¤.
ìœ„ íŒ¨í„´ì´ ë§ìœ¼ë ¤ë©´? 
ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒê°
ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì¼ ë•Œ, í•´ë‹¹ íŒ¨í„´ì´ ìì£¼ ë‚˜íƒ€ë‚œë‹¤
í…Œë‹ˆìŠ¤ ìì²´ë¥¼ ë§ì´ ì¹˜ëŠ” ê²½ìš°
í…Œë‹ˆìŠ¤ë¥¼ ì³¤ì„ ë•Œ, í•´ë‹¹ ê²‰ë³´ê¸° ë‚ ì”¨ì™€ ìŠµë„ê°€ ìì£¼ ì¶œí˜„í•œ ê²½ìš°
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ” ì„¤ëª… ë³€ìˆ˜ ê°„ì˜ ë…ë¦½ì„ ê°€ì •
í…Œë‹ˆìŠ¤ë¥¼ ì³¤ì„ ë•Œ, ê²‰ë³´ê¸° ë‚ ì”¨ì™€ ìŠµë„ëŠ” ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ìŒì„ ê°€ì •

2. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ : Naive bayes classifier ìˆ˜í•™ì  ê°œë… ì´í•´ : ì¡°ê±´ë¶€ í™•ë¥ , ë² ì´ì¦ˆ ì •ë¦¬
ìˆ˜í•™ì  ê°œë… ì´í•´ : ì¡°ê±´ë¶€ í™•ë¥ 
íŠ¹ì •, ê²‰ë³´ê¸° ë‚ ì”¨ì™€ ìŠµë„ê°€ ê³ ì •ë˜ì–´ ìˆë‹¤ê³  í•  ë•Œ, í…Œë‹ˆìŠ¤ë¥¼ ì¹  í™•ë¥ ì´ ë†’ìœ¼ë ¤ë©´?
íŠ¹ì •, ê²‰ë³´ê¸° ë‚ ì”¨ì™€ ìŠµë„ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í…Œë‹ˆìŠ¤ë¥¼ ì¹  ì¡°ê±´ë¶€ í™•ë¥ ì´ ë†’ì•„ì•¼ í•œë‹¤.
ìˆ˜í•™ì  ê°œë… ì´í•´ : ë² ì´ì¦ˆ ì •ë¦¬
ìœ„ íŒ¨í„´ì´ ë§ìœ¼ë ¤ë©´?
ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒê°
ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì¼ ë•Œ, í•´ë‹¹ íŒ¨í„´ì´ ìì£¼ ë‚˜íƒ€ë‚œë‹¤.
í…Œë‹ˆìŠ¤ ìì²´ë¥¼ ë§ì´ ì¹˜ëŠ” ê²½ìš°
í…Œë‹ˆìŠ¤ë¥¼ ì³¤ì„ ë•Œ, í•´ë‹¹ ë‚ ì”¨ì™€ ìŠµë„ê°€ ìì£¼ ì¶œí˜„í•œ ê²½ìš°

3. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ : Naive bayes classifier ì¢…ë¥˜ ë° ì´í•´
Gaussian Naive bayes classifier : ì„¤ëª… ë³€ìˆ˜ê°€ ì—°ì†í˜•ì¸ ê²½ìš°
Multinormal Naive bayes classifier : ì„¤ëª… ë³€ìˆ˜ê°€ ë²”ì£¼í˜•ì¸ ê²½ìš°

3-1. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì‹¤ìŠµ

### 1. Gaussian Naive Bayes

- ë°ì´í„°, ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°

from sklearn import datasets
from sklearn.naive_bayes import GaussianNB
import pandas as pd

iris = datasets.load_iris()
df_X=pd.DataFrame(iris.data)
df_Y=pd.DataFrame(iris.target)

df_X.head()
df_Y.head()

- ëª¨ë¸ í”¼íŒ…

gnb = GaussianNB()
fitted = gnb.fit(iris.data, iris.target)
y_pred = fitted.predict(iris.data)

fitted.predict_proba(iris.data)[[1, 48, 51, 100]]

fitted.predict(iris.data)[[1, 48, 51, 100]]

- Confusion matrix êµ¬í•˜ê¸°

from sklearn.metrics import confusion_matrix

confusion_matrix(iris.target,y_pred)

- Prior ì„¤ì •í•˜ê¸°

gnb2 = GaussianNB(priors = [1/100, 1/100, 98/100])
fitted2 = gnb2.fit(iris.data, iris.target)
y_pred2 = fitted2.predict(iris.data)
confusion_matrix(iris.target, y_pred2)

gnb2 = GaussianNB(priors = [1/100, 98/100, 1/100])
fitted2 = gnb2.fit(iris.data, iris.target)
y_pred2 = fitted2.predict(iris.data)
confusion_matrix(iris.target, y_pred2)

### 2. Multinomial naive bayes

- ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ë° ë°ì´í„° ìƒì„±

from sklearn.naive_bayes import MultinomialNB
import numpy as np

X = np.random.randint(5, size=(6, 100))
y = np.array([1, 2, 3, 4, 5, 6])

- Multinomial naive bayes ëª¨ë¸ ìƒì„±

clf = MultinomialNB()
clf.fit(X, y)

print(clf.predict(X[2:3]))

clf.predict_proba(X[2:3])

- prior ë³€ê²½í•´ë³´ê¸°

clf2 = MultinomialNB(class_prior = [0.1, 0.1999, 0.0001, 0.1, 0.1, 0.1])
clf2.fit(X, y)

clf2.predict_proba(X[2:3])

4. KNN (K-Nearest neighborhood) ë°°ê²½
KëŠ” ì–´ë–»ê²Œ ì •í•˜ëŠ” ê°€?
ë„ˆë¬´ í° K : ë¯¸ì„¸í•œ ê²½ê³„ ë¶€ë¶„ì„ ì˜ëª» ë¶„ë¥˜í•  ê²ƒ
ë„ˆë¬´ ì‘ì€ K : ì´ìƒì¹˜ì˜ ì˜í–¥ì„ í¬ê²Œ ë°›ì„ ê²ƒ, íŒ¨í„´ì´ ì§ê´€ì ì´ì§€ ì•Šì„ ê²ƒ
ì¤‘ìš”í•œ ë³€ìˆ˜ì™€ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ê°€ ì„ì—¬ ìˆë‹¤ë©´? ì¤‘ìš”í•œ ë³€ìˆ˜ë¥¼ ì„ ë³„í•  í•„ìš”ê°€ ìˆìŒ
ì¢…ì† ë³€ìˆ˜
ë²”ì£¼í˜• ë³€ìˆ˜
KNN ì¤‘ ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ë²”ì£¼ë¡œ yë¥¼ ì¶”ì •
Tie ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´ KëŠ” í™€ìˆ˜ë¡œ ì •í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤
ì—°ì†í˜• ë³€ìˆ˜
KNNì˜ ëŒ€í‘œê°’(í‰ê· )ìœ¼ë¡œ yë¥¼ ì¶”ì²­
ê±°ë¦¬ëŠ” ì–´ë–»ê²Œ êµ¬í•˜ë‚˜?
ì„¤ëª… ë³€ìˆ˜
ë²”ì£¼í˜• ë³€ìˆ˜ : Hamming distance
ì—°ì†í˜• ë³€ìˆ˜ : ìœ í´ë¦¬ì•ˆ ê±°ë¦¬, ë§¨í•˜íƒ„ ê±°ë¦¬

5. KNN (K-Nearest neighborhood) ëª¨ë¸ ë° ì •ì˜, Cross validation
ê³¼ì í•©ì˜ ë¬¸ì œ
Training setì„ ê°€ì¥ ì˜ ë§íˆëŠ” ë¨¸ì‹ ì€ Test setì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤
Training errorëŠ” errorë¥¼ ê³¼ì†Œì¶”ì •í•˜ëŠ” ì„±í–¥ì´ ìˆìŒ

6. KNN ì •ë¦¬
Kì˜ ê²°ì •
ë„ˆë¬´ í° K : ë¯¸ì„¸í•œ ê²½ê³„ ë¶€ë¶„ì„ ì˜ëª» ë¶„ë¥˜í•  ê²ƒ
ë„ˆë¬´ ì‘ì€ K : ì´ìƒì¹˜ì˜ ì˜í–¥ì„ í¬ê²Œ ë°›ì„ ê²ƒ, íŒ¨í„´ì´ ì§ê´€ì ì´ì§€ ì•Šì„ ê²ƒ
Training error : k = 1ì—ì„œ ê°€ì¥ ë‚®ìŒ, ê³¼ì í•©ì˜ ê°€ëŠ¥ì„±
Test error : ë°ì´í„°ì— ë”°ë¼ ìµœì ì˜ kê°€ ì¡´ì¬
Kì˜ ê²°ì • : Test errorë¥¼ ì‘ê²Œí•˜ëŠ” k, cross-validation ì´ìš©
ì°¨ì›ì˜ ì €ì£¼
Yì¶• ì •ë³´ê°€ ì¶”ê°€ë¨ì— ë”°ë¥¸ ë¹„íš¨ìœ¨
ì°¨ì› ì¶•ì†Œ : Principal Component Analysis, Canonical Correlation Analysis ë“±ì˜ ë°©ë²•ì„ í™œìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•œ í›„ KNN ì§„í–‰

6-1. KNN ì‹¤ìŠµ

### 1. ë°ì´í„°, ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ë° kNN í”¼íŒ… ë°©ë²•

- í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

from sklearn import neighbors, datasets
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.metrics import confusion_matrix

iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

- ëª¨ë¸ êµ¬ì¶•

clf = neighbors.KNeighborsClassifier(5)
clf.fit(X, y)

y_pred=clf.predict(X)

confusion_matrix(y,y_pred)

### 2.Cross-validationì„ í™œìš©í•œ ìµœì ì˜ kì°¾ê¸°

- í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
- "from sklearn.cross_validation import cross_val_score" ì½”ë“œê°€ ì•„ë˜ì™€ ê°™ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

from sklearn.model_selection import cross_val_score

- CV ì§„í–‰

k_range = range(1, 100)
k_scores = []

for k in k_range:
    knn = neighbors.KNeighborsClassifier(k)
    scores = cross_val_score(knn, X, y, cv = 10, scoring='accuracy')
    k_scores.append(scores.mean())

plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-validated accuracy')
plt.show()

### 2.Weightë¥¼ ì¤€ kNN

n_neighbors = 40

h = .02  # step size in the mesh

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
[O    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()

np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()
y[::5] += 1 * (0.5 - np.random.rand(8))

knn = neighbors.KNeighborsRegressor(n_neighbors)
y_ = knn.fit(X, y).predict(T)

n_neighbors = 5

for i, weights in enumerate(['uniform', 'distance']):
    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)
    y_ = knn.fit(X, y).predict(T)

    plt.subplot(2, 1, i + 1)
    plt.scatter(X, y, c='k', label='data')
    plt.plot(T, y_, c='g', label='prediction')
    plt.axis('tight')
    plt.legend()
    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,
                                                                weights))

plt.tight_layout()
plt.show()
