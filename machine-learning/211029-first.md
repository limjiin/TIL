Part.03 ê¸°ë³¸ì ì¸ ë¨¸ì‹  ëŸ¬ë‹ ëª¨í˜•
1.  LDA (Linear Discriminant Analysis)
ê°€ì •
ê° ìˆ«ì ì§‘ë‹¨ì€ ì •ê·œ ë¶„í¬ í˜•íƒœì˜ í™•ë¥ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤
ê° ìˆ«ì ì§‘ë‹¨ì€ ë¹„ìŠ·í•œ í˜•íƒœì˜ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.
LDA ê²°ê³¼ ì–»ê²Œ ë˜ëŠ” decision boundary íŠ¹ì§•
boundaryì— ì§êµí•˜ëŠ” ì¶• : ìë£Œë“¤ì„ ì´ ì¶•ì— ì •ì‚¬ì˜ ì‹œí‚¨ ë¶„í¬ì˜ í˜•íƒœë¥¼ ê³ ë ¤
í‰ê· ì˜ ì°¨ì´ë¥¼ ê·¹ëŒ€í™” í•˜ë ¤ë©´? ë‘ í‰ê·  vectorì˜ ì°¨ì´ ë²¡í„°ë¥¼ ì´ìš©
ì •ì‚¬ì˜ ì‹œí‚¨ ë‘ ë¶„í‘œì˜ íŠ¹ì§•ì´ ì•„ë˜ ë‘˜ì„ ë™ì‹œì— ë‹¬ì„±í•˜ê³ ì í•¨
í‰ê· ì˜ ì°¨ì´ëŠ” ìµœëŒ€í™”
ë‘ ë¶„í¬ì˜ ê°ê° ë¶„ì‚°ì€ ìµœì†Œí™”
ê²°êµ­ ë¶„ì‚° ëŒ€ë¹„ í‰ê· ì˜ ì°¨ì´ë¥¼ ê·¹ëŒ€í™” í•˜ëŠ” boundaryë¥¼ ì°¾ê³ ì í•˜ëŠ” ê²ƒ

2. LDA ìˆ˜í•™ì  ê°œë… ì´í•´ : ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬
ì´ë³€ëŸ‰ ì •ê·œë¶„í¬
ì •ê·œë¶„í¬
ì´ë³€ëŸ‰ ì •ê·œë¶„í¬
ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ 
ë¡œê·¸ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬
ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ì˜ LDAì—ì˜ ì ìš©
Kë²ˆì§¸ ë²”ì£¼ ìë£Œì˜ ë¶„í¬í•¨ìˆ˜
Kë²ˆì§¸ ë²”ì£¼ê°€ ë‚˜íƒ€ë‚  ì‚¬ì „ í™•ë¥ ì„ ê³ ë ¤í•˜ë©´
Kë²ˆì§¸, 1ë²ˆì§¸ ë²”ì£¼ì—ì„œ í˜„ì¬ ìë£Œê°€ ë‚˜ì™”ì„ í™•ë¥ 
ë§ˆë¬´ë¦¬
LDA decision boundary : í‰ë©´ê³¼ ì´ë¥¼ ê°€ë¡œì§€ë¥´ëŠ” ì ì„ 
ê³µë¶„ì‚°ì´ ê°™ë‹¤ëŠ” êµ¬ì¡° ì•„ë˜ LDAê°€ 0 ~ 1 ì‚¬ì´ì—ì„œ Kì¸ì§€ Lì¸ì§€ ì•Œ ìˆ˜ ìˆë‹¤.
Xì˜ ì´ë‹ˆ í˜•ì´ ë“¤ì–´ê°€ê³  ì´ê²ƒì´ ì´ˆí‰ë©´ í˜•íƒœ

3. LDA ëª¨ë¸ ì •ì˜ ë° ì¶”ì •
LDA ëª¨ë¸ ì •ì˜ 
LDA decision boundary
ë¶„ì‚° ëŒ€ë¹„ í‰ê· ì˜ ì°¨ì´ë¥¼ ê·¹ëŒ€í™” í•˜ëŠ” boundary
ê°€ì •
ê° ìˆ«ì ì§‘ë‹¨ì˜ ì •ê·œë¶„í¬ í˜•íƒœì˜ í™•ë¥  ë¶„í¬ë¥¼ ê°€ì§„ë‹¤
ê° ìˆ«ì ì§‘ë‹¨ì€ ë¹„ìŠ·í•œ í˜•íƒœì˜ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤
í™•ë¥ ë¶„í¬ ê´€ì 
Yê°€ Kê°œì˜ ë²”ì£¼ë¥¼ ê°€ì§ˆ ë•Œ
Y = K ì¼ ë•Œ, ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” pê°œì˜ ì •ê·œë¶„í¬ ë³€ìˆ˜ì˜ ë¶„í¬
Kì™€ ê´€ê³„ ì—†ëŠ” ê³µí†µ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§ìœ¼ë¡œ ì¸í•´ì„œ xì— ëŒ€í•œ 1ì°¨ì‹(linear)ìœ¼ë¡œ ì •ë¦¬ëœë‹¤

4. LDA ìˆ˜í•™ì  ê°œë… ì´í•´ : ì‚¬ì˜ (Projection)
ì‚¬ì˜ (Projection)
ëª©í‘œ : ë¶„ì‚°ì€ ìµœì†Œí™”í•˜ë©´ì„œ í‰ê· ì„ ìµœëŒ€í™” í•˜ëŠ” ì‚¬ì˜ì„ ì°¾ëŠ” ê²ƒ
ì‚¬ì˜ëœ ê³³ì—ì„œ í‰ê· ì˜ ì°¨ì´ì™€ ë¶„ì‚°ì„ í‘œí˜„í•´ì•¼ í•¨
ì´ì •ë¦¬
ì•„ì´ë””ì–´, ë¶„ì‚°ì„ ìµœì†Œí™” í‰ê· ì˜ ì°¨ì´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì¶•ì— ìˆ˜ì§ì¸ boundaryë¥¼ ì°¾ê³ ì í•¨
íˆ¬ì˜ì„ í†µí•´ ì°¾ì•„ë‚¸ ìƒˆë¡œìš´ ì¶• a = eigen vector
í™•ë¥  ëª¨ë¸ë¡œ ì°¾ì•„ë‚¸ decision boundary
ì¥ì 
ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ê³¼ ë‹¬ë¦¬, ì„¤ëª…ë³€ìˆ˜ ê°„ì˜ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ë°˜ì˜
ê°€ì •ì´ ìœ„ë°˜ë˜ë”ë¼ë„ ë¹„êµì  robust
ë‹¨ì 
ê°€ì¥ ì‘ì€ ê·¸ë£¹ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ì„¤ëª… ë³€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ë§ì•„ì•¼ í•¨
ì •ê·œë¶„í¬ ê°€ì •ì— í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ì˜ ì„¤ëª…í•˜ì§€ ëª»í•¨
y ë²”ì£¼ ì‚¬ì´ì— ê³µë¶„ì‚° êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨

5. LDA ì‹¬í™”ì  ì´í•´
QDA ì •ì˜ ë° ì´í•´
Kì™€ ê´€ê³„ ì—†ëŠ” ê³µí†µ ê³µë¶„ì‚° êµ¬ì¡°ì— ëŒ€í•œ ê°€ì •ì„ ë²„ë¦° ê²ƒì´ QDA
yì˜ ë²”ì£¼ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§„ ê²½ìš°ì— í™œìš© ê°€ëŠ¥
ì¥ì 
yë²”ì£¼ë³„ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ë‹¤ë¥´ê²Œ í•  ìˆ˜ ìˆìŒ
ë‹¨ì 
ì…œëª…ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ë§ì„ ê²½ìš°, ì¶”ì •í•´ì•¼ í•˜ëŠ” ëª¨ìˆ˜ê°€ ë§ì•„ì§
ìƒ˜í”Œì´ ë§ì´ í•„ìš”

5-1. LDA (Linear Discriminant Analysis) ì‹¤ìŠµ

##### 1. Linear Discriminant Analysis

- LDA ë¥¼ ìœ„í•œ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])

- LDA ëª¨ë¸ êµ¬ì¶•

clf = LinearDiscriminantAnalysis()
clf.fit(X, y)

print(clf.predict([[-0.8, -1]]))

##### 2. Quadratic Discriminant Analysis

- QDAë¥¼ ìœ„í•œ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

- QDA ëª¨ë¸ êµ¬ì¶•

clf2 = QuadraticDiscriminantAnalysis()
clf2.fit(X, y)

print(clf2.predict([[-0.8, -1]]))

- LDA, QDA ë¹„êµ

from sklearn.metrics import confusion_matrix
y_pred=clf.predict(X)
confusion_matrix(y,y_pred)

y_pred2=clf2.predict(X)
confusion_matrix(y,y_pred2)

##### 3. LDA, QDAì˜ ì‹œê°ì  ë¹„êµ

from sklearn.datasets import make_moons, make_circles, make_classification
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

h=0.2
names = ["LDA", "QDA"]
classifiers = [
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable
            ]

figure = plt.figure(figsize=(27, 9))
i = 1
# iterate over datasets
for ds in datasets:
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
[O    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, m_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   alpha=0.6)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

figure.subplots_adjust(left=.02, right=.98)
plt.show()

6. SVM (Support Vector Machine)
ë°°ê²½
ë°ì´í„°ì˜ ë¶„í¬ê°€ì •ì´ í˜ë“¤ ë•Œ, ì•„ë˜ì˜ ë°ì´í„°ë¥¼ ì˜ ë‚˜ëˆ„ë ¤ë©´?
Boundaryì— ì§‘ì¤‘
ë¬¸ì œì 
ì •í™•íˆ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ì¡´ì¬í•œë‹¤ë©´?
ì ë‹¹í•œ errorë¥¼ í—ˆìš©í•˜ê³ , ì´ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ boundaryë¥¼ ê²°ì •
ì¢…ì† ë³€ìˆ˜ ë°ì´í„° í˜•íƒœì— ë”°ë¼ ë‚˜ë‰¨
ë²”ì£¼í˜• ë³€ìˆ˜ : Support Vector classifier
ì—°ì†í˜• ë³€ìˆ˜ : Support Vector regression
SVM, SVRì˜ í•µì‹¬
Model costì— ì˜í–¥ì„ ë¼ì¹  ì ê³¼ ë¼ì¹˜ì§€ ì•Šì„ ì ì„ marginì„ í†µí•´ êµ¬ë¶„
SVM : ë§ˆì§„ ì•ˆì— í¬í•¨ë˜ê±°ë‚˜, ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë¶„ë¥˜ëœ ì  ë“¤
SVR : ë§ˆì§„ ë°”ê¹¥ì— ìœ„ì¹˜í•œ ì ë“¤
decision boundary/ rule
Lagrange multiplier : ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜
ìµœì í™” ë¬¸ì œ(ìµœì†Œí™” ë˜ëŠ” ìµœëŒ€í™”í•˜ëŠ” ê°’)ë¥¼ í’€ ë•Œ, ìµœëŒ€í™”í•˜ëŠ” ë™ì‹œì— í•œì •í•˜ê³  ì‹¶ì€ ê²½ìš°
ì¥ì  vs LDA
ë°ì´í„°ì˜ ë¶„í¬ ê°€ì •ì´ í˜ë“  ê²½ìš°, covariance êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì 
Boundary ê·¼ì²˜ì˜ ê´€ì¸¡ì¹˜ë§Œì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ
ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë†’ìŒ
ë‹¨ì 
Cë¥¼ ê²°ì •í•´ì•¼ í•¨
ëª¨í˜• êµ¬ì¶•ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
One-Class  SVM (Support Vector Machine)
ì¢…ì†ë³€ìˆ˜ ì •ë³´ê°€ ì—†ëŠ” ìë£Œë¥¼ ìš”ì•½í•˜ëŠ” ë° SVM ì‚¬ìš©
ìë£Œë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì›ì„ í™œìš©

6-1. SVM (Support vector machine) ì‹¤ìŠµ

##### 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ë° SVM ì í•©

- í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

- ëª¨ë¸ ì í•©

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

C = 1
clf = svm.SVC(kernel = 'linear', C=C)
clf.fit(X, y)

from sklearn.metrics import confusion_matrix

y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

##### 2. kernel SVM ì í•© ë° ë¹„êµ

- LinearSVC

clf = svm.LinearSVC(C=C, max_iter=10000)
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

- radial basis function

clf = svm.SVC(kernel = 'rbf', gamma = 0.7, max_iter=10000)
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

- polynomial kernel

clf = svm.SVC(kernel = 'poly', degree=3, C=C, gamma='auto')
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

##### ì‹œê°ì  ë¹„êµ

- í•¨ìˆ˜ ì •ì˜

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy


def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

- ëª¨ë¸ì •ì˜ ë° í”¼íŒ…

C = 1.0 #Regularization parameter
models = (svm.SVC(kernel='linear', C=C),
          svm.LinearSVC(C=C, max_iter=10000),
          svm.SVC(kernel='rbf', gamma=0.7, C=C),
          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))
models = (clf.fit(X, y) for clf in models)

titles = ('SVC with linear kernel',
          'LinearSVC (linear kernel)',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel')

fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy,
                  cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel('Sepal length')
    ax.set_ylabel('Sepal width')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)

plt.show()

7. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ë°°ê²½
ë³€ìˆ˜ë“¤ë¡œ ê¸°ì¤€ì„ ë§Œë“¤ê³  ì´ê²ƒì„ í†µí•˜ì—¬ ìƒ˜í”Œì„ ë¶„ë¥˜í•˜ê³  ë¶„ë¥˜ëœ ì§‘ë‹¨ì˜ ì„±ì§ˆì„ í†µí•˜ì—¬ ì¶”ì •í•˜ëŠ” ëª¨í˜•
ì¥ì  : í•´ì„ë ¥ì´ ë†’ìŒ, ì§ê´€ì , ë²”ìš©ì„±
ë‹¨ì  : ë†’ì€ ë³€ë™ì„±, ìƒ˜í”Œì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ
ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ìš©ì–´
Node - ë¶„ë¥˜ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ë³€ìˆ˜ê°€ ìœ„ì¹˜ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œì„ ë‚˜ëˆ”
Parent Node - í•˜ìœ„ ë…¸ë“œ
Root Node - ìƒìœ„ ë…¸ë“œê°€ ì—†ëŠ” ê°€ì¥ ìœ„ì˜ ë…¸ë“œ
Leaf node (Tip) - í•˜ìœ„ ë…¸ë“œê°€ ì—†ëŠ” ê°€ì¥ ì•„ë˜ ë…¸íŠ¸
Internal node - Leaf nodeê°€ ì•„ë‹Œ ë…¸ë“œ
Edge - ìƒ˜í”Œì„ ë¶„ë¥˜í•˜ëŠ” ì¡°ê±´ì´ ìœ„ì¹˜í•˜ëŠ” ê³³
Deepth - Root nodeì—ì„œ íŠ¹ì • ë…¸ë“œê¹Œì§€ ë„ë‹¬í•˜ê¸° ìœ„í•´ ê±°ì³ì•¼ í•˜ëŠ” Edgeì˜ ìˆ˜
ë°˜ì‘ ë³€ìˆ˜ì— ë”°ë¼
ë²”ì£¼í˜• ë³€ìˆ˜ : ë¶„ë¥˜ íŠ¸ë¦¬
ì—°ì†í˜• ë³€ìˆ˜ : íšŒê·€ íŠ¸ë¦¬ 

8. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ìˆ˜í•™ì  ê°œë… ì´í•´
ì—”íŠ¸ë¡œí”¼ (Entropy)
ì§ê´€ì  ì •ì˜ : 0 ë˜ëŠ” 1ì¼ í™•ë¥ ì´ ìµœì†Œ, 0.5ì¼ í™•ë¥ ì´ ìµœëŒ€ê°€ ë˜ê²Œ í•˜ëŠ” í•¨ìˆ˜
Information Gain
Information Gain = Entropy before - Entropy after
Decision Treeì˜ íŠ¹ì • node ì´ì „ê³¼ ì´í›„ì˜ Entropy ì°¨ì´
Classification Tree
Tree ì¡°ê±´ì— ë”°ë¼, Xê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì˜ì—­ì„ Blockìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê°œë…
ë‚˜ëˆ„ì–´ì§„ ì˜ì—­ ì•ˆì— ì†í•˜ëŠ” ìƒ˜í”Œì˜ íŠ¹ì„±ì„ í†µí•˜ì—¬ Yë¥¼ ì¶”ì •
Rmì˜ êµ¬ì„±
ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•˜ì—¬,
ë²”ì£¼í˜• : ê° ë²”ì£¼ì— ë”°ë¼
ì—°ì†í˜• : ì—¬ëŸ¬ ê°œì˜ ì˜ì—­ìœ¼ë¡œ ì„ì˜ë¡œ ë‚˜ëˆ”
ë‚˜ëˆ„ì–´ë‘” ì˜ì—­ë“¤ì— ëŒ€í•´, ì•„ë˜ measureë¥¼ ê°€ì¥ ì¢‹ì€ ê°’ìœ¼ë¡œ ë§Œë“œëŠ” ë³€ìˆ˜ì™€ ê¸°ì¤€ì„ ì„ íƒ
ì—”íŠ¸ë¡œí”¼
ì˜¤ë¶„ë¥˜ìœ¨
Regression Tree
Tree ì¡°ê±´ì— ë”°ë¼, Xê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì˜ì—­ì„ Blockìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê°œë…
ë‚˜ëˆ„ì–´ì§„ ì˜ì—­ ì•ˆì— ì†í•˜ëŠ” ìƒ˜í”Œì˜ íŠ¹ì„±ì„ í†µí•˜ì—¬ Yë¥¼ ì¶”ì •

8-1. ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree) ì‹¤ìŠµ

##### 1. í•¨ìˆ˜ ìµíˆê¸° ë° ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°

- í•¨ìˆ˜ ìµíˆê¸°

from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[1, 1]])

- ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°

from sklearn.datasets import load_iris
from sklearn import tree
from os import system

system("pip install graphviz")

import graphviz

- ë°ì´í„° ë¡œë“œ

iris = load_iris()

##### 2. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ êµ¬ì¶• ë° ì‹œê°í™”

- íŠ¸ë¦¬ êµ¬ì¶•

clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)

- íŠ¸ë¦¬ì˜ ì‹œê°í™”

dot_data = tree.export_graphviz(clf,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph=graphviz.Source(dot_data)

- ì—”íŠ¸ë¡œí”¼ë¥¼ í™œìš©í•œ íŠ¸ë¦¬

clf2 = tree.DecisionTreeClassifier(criterion='entropy')
clf2 = clf.fit(iris.data, iris.target)

dot_data2 = tree.export_graphviz(clf2,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph2 = graphviz.Source(dot_data2)

- í”„ë£¨ë‹

clf3 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)

clf3.fit(iris.data, iris.target)

dot_data3 = tree.export_graphviz(clf3,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph3 = graphviz.Source(dot_data3)

- Confusion Matrix êµ¬í•˜ê¸°

from sklearn.metrics import confusion_matrix
confusion_matrix(iris.target,clf.predict(iris.data))

confusion_matrix(iris.target,clf2.predict(iris.data))

confusion_matrix(iris.target,clf3.predict(iris.data))

##### 3. Training - Test êµ¬ë¶„ ë° Confusion matrix ê³„ì‚°

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(iris.data,
                                                    iris.target,
                                                    stratify=iris.target,
                                                    random_state=1)

clf4 = tree.DecisionTreeClassifier(criterion='entropy')

clf4.fit(X_train, y_train)

confusion_matrix(y_test, clf4.predict(X_test))

##### 4. Decision regression tree

- ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ë° ë°ì´í„° ìƒì„±

import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

- Regression tree êµ¬ì¶•

regr1 = tree.DecisionTreeRegressor(max_depth=2)
regr2 = tree.DecisionTreeRegressor(max_depth=5)

regr1.fit(X, y)
regr2.fit(X, y)

X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
X_test

y_1 = regr1.predict(X_test)
y_1

y_2 = regr2.predict(X_test)
y_2

plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",
            c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",
         label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()

dot_data4 = tree.export_graphviz(regr2, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True)

graph4 = graphviz.Source(dot_data4)
graph4

dot_data5 = tree.export_graphviz(regr1, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True)

graph5 = graphviz.Source(dot_data5)

9. ì‹ ê²½ë§ ëª¨í˜•
ë°°ê²½ : ì¸ê°„ì˜ ë‰´ëŸ°
ì‹œëƒ…ìŠ¤ë¥¼ í†µí•˜ì—¬ ì—¬ëŸ¬ ë‰´ëŸ°ìœ¼ë¡œ ë¶€í„° ìê·¹ì„ ì „ë‹¬ ë°›ìŒ
ì´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ë¥¸ ë‰´ëŸ°ì—ê²Œ ìê·¹ì„ ì „ë‹¬
 ë‰´ëŸ°ì˜ êµ¬ì¡°ë¥¼ ëª¨ë¸ë§
í•˜ë‚˜ì˜ ë‰´ëŸ° : perceptron
ì—¬ëŸ¬ layerë¥¼ í†µí•˜ì—¬ ì‹œëƒ…ìŠ¤ë¥¼ í‘œí˜„
Perceptron
í•˜ë‚˜ì˜ ë‰´ëŸ°
ì…ë ¥ ë°ì´í„° í˜¹ì€ ë‹¤ë¥¸ ë ˆì´ì–´ì˜ ì¶œë ¥ë¬¼ì„ ë°›ì•„ ê²°ê³¼ê°’ì„ ë‚´ëŠ” êµ¬ì¡°
input, weights, activation function(í™œì„±í•¨ìˆ˜)ë¡œ êµ¬ì„±
Multi Layer Perceptron
í•˜ë‚˜ì˜ Hidden layer(ì€ë‹‰ë…¸ë“œ) : 4 perceptron
2ê°œì˜ ì¢…ì†ë³€ìˆ˜ : 2 perceptron
ì¸ì¡° ë‰´ëŸ°ì˜ êµ¬ì¡°
Activation function : í™œì„±í•¨ìˆ˜
ì—°ì†, ë¹„ì„ í˜•, ë‹¨ì¡°ì¦ê°€, bounded, ì ê·¼ì„±ì˜ íŠ¹ì„±
ê°€ì¥ ê¸°ë³¸ì ì¸ : step function, sigmoid function(ë¯¸ë¶„ì´ ì‰¬ì›€. ë”¥ëŸ¬ë‹ ì´ì „ì— ë§ì´ ì‚¬ìš©í•¨)
í•„ìš”ì„± : ì€ë‹‰ layerì„ ì˜ë¯¸ ìˆê²Œ ìŒ“ì•„ì£¼ëŠ” ì—­í• 
ì„ í˜•ì˜ layerë§Œ ìŒ“ì¸ë‹¤ë©´ ê²°êµ­ í•˜ë‚˜ì˜ ì„ í˜•ì‹ì´ ë¨
ì¶œë ¥ê°’ì˜ rangeë¥¼ ê²°ì •
ì‹ ê²½ë§ ëª¨í˜•ì˜ êµ¬ì¡° ì´í•´
input layer(ì…ë ¥ì¸µ) : ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ë¯¸
hidden layer : ì…ë ¥ê°’ : ì…ë ¥ ë°ì´í„° í˜¹ì€ ë˜ ë‹¤ë¥¸ hidden layerì˜ ì¶œë ¥ê°’ : ìœ„ì˜ ì…ë ¥ ê°’ì„ ë°›ëŠ” perceptronë“¤ì˜ ì§‘í•©
output layer : ì…ë ¥ê°’ : ë§ˆì§€ë§‰ hidden layerì˜ ì¶œë ¥ê°’ : ìµœì¢… ê²°ê³¼ë¬¼ì„ ë§Œë“¤ì–´ë‚´ëŠ” perceptronë“¤ì˜ ì§‘í•©
ìˆ˜í•™ì  ê°œë… ì´í•´ : ë°”ì´ë„ˆë¦¬ ë…¼ë¦¬ì—°ì‚°ê³¼ perceptron
ë°”ì´ë„ˆë¦¬ ë…¼ë¦¬ ì—°ì‚° : AND, OR, XOR ì—°ì‚°ì´ ì¡´ì¬
ë‹¨ì¸µ perceptron (single-layer perceptron)
ì†ì„±ì´ 2ê°œì¸ ê²½ìš°, í‰ë©´ìƒì˜ ì§ì„ ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥
perceptronì˜ classification
ì¸ì¡° ë‰´ëŸ° OR ì—°ì‚° êµ¬ì¶• - ì•Œê³ ë¦¬ì¦˜ì˜ ì´í•´
Weight ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜
LR : weightë¥¼ ë³€í™”ì‹œí‚¤ëŠ” ì •ë„
ê°’ì´ ë„ˆë¬´ í¬ë©´ ì •í™•í•œ í•´ë¥¼ ì°¾ê¸° í˜ë“¦
ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´í•˜ê¸°ê¹Œì§€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
E : ì •ì˜ëœ error ê°’. ì£¼ë¡œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ì‚¬ìš©
Backpropagation (ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜)
Multi layer perceptronì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°©ë²•
Output layerì—ì„œì˜ errorì˜ ë³€í™”ëŸ‰ì„ ì•ì„  layerë“¤ë¡œ ì „íŒŒí•œë‹¤ëŠ” ê°œë…
ë¯¸ë¶„ì„ í†µí•´ ì ‘ê·¼
ì‹ ê²½ë§ ëª¨í˜•ì˜ í•œê³„ì 
gradient vanishing
sigmoid í•¨ìˆ˜ì˜ í•œê³„ì 
ì¤‘ê°„í•´ ë©ˆì¶¤ í˜„ìƒ
ìµœì í•´(Global minima)ì— ì´ë¥´ê¸° ì „ì— ì¤‘ê°„í•´(Local minima)ì—ì„œ ë©ˆì¶”ëŠ” í˜„ìƒ
ê³¼ì í•© ë¬¸ì œ
Training setì— ê³¼í•˜ê²Œ ìµœì í™” ë˜ëŠ” ë¬¸ì œ
ì¼ë°˜í™” ë˜ì§€ ì•ŠìŒ
ì‹ ê²½ë§ ëª¨í˜•ì˜ í•´ê²°ì±…
ReLU
ê³„ì‚°ì´ ê°„ë‹¨ : í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„
0 ë³´ë‹¤ í° ê²½ìš° ê¸°ìš¸ê¸° ìœ ì§€
0ë³´ë‹¤ ì‘ì€ ê²½ìš° ê¸°ìš¸ê¸°ê°€ ì—†ëŠ” ë¬¸ì œì ì€ ë‹¤ë¥¸ ReLU ê³„ì—´ í•¨ìˆ˜ ì´ìš©
Pre-training : ë¯¸ë¦¬ traning ì‹œì¼œì„œ local minima ë¬¸ì œë¥¼ í•´ê²°. ì˜¬ë°”ë¥¸ ì´ˆê¸°ê°’ ì„ ì •ì— ë„ì›€
Drop outì„ í†µí•œ ê³¼ì í•© ë¬¸ì œ ì™„í™”
Hidden layerì˜ nodeë¥¼ ì„ì˜ì˜ í™•ë¥ ì— ë”°ë¼ ë‚¨ê¹€ (0.5~1 ì‚¬ì´ì˜ í™•ë¥ ì„ ê¶Œì¥)
ê³„ì‚° ì†ë„ë„ ì¦ê°€
ì‹ ê²½ë§ ëª¨í˜•ì˜ ì‹¬í™”ì  ì´í•´
ì´ˆê¸°ê°’ ë¬¸ì œ
Restricted Boltzmann Machine
Yë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  Xë§Œì„ ì‚¬ìš©í•˜ì—¬ weigthë¥¼ í•™ìŠµ
ì„œë¡œ ì¸ì ‘í•œ ì¸µ ì‚¬ì´ì—ì„œë§Œ í•™ìŠµí•˜ì—¬ ì„œë¡œ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡í•œ ê°’ì´ ìµœì†Œê°€ ë˜ëŠ” weightë¥¼ ì°¾ìŒ
ì´ ê°’ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„ ì •
ê³¼ì í•©
weight decay
ê°€ì¤‘ì¹˜ê°€ ìµœëŒ€í•œ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ penalty ë¶€ì—¬
Ridge regressionê³¼ ê°™ì€ ì•„ì´ë””ì–´
Activation functionì˜ ì„ íƒ
ReLUì˜ ë‹¨ì  ë³´ì™„ : ELU
ì€ë‹‰ ì¸µ ê°œìˆ˜, ì€ë‹‰ ë…¸ë“œì˜ ê°œìˆ˜, ì˜ë¯¸
ì€ë‹‰ ë…¸ë“œë¥¼ ì§€ë‚˜ì¹˜ê²Œ ë§ì´ ëŠ˜ë¦¬ë©´ ê³¼ì í•©ì˜ ë¬¸ì œ ë°œì„±
ë³µì¡í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²½ìš° > ì¶©ë¶„í•œ ì€ë‹‰ ì¸µì˜ ê°œìˆ˜ í•„ìš”
ë‹¤ì–‘í•œ ì…ë ¥ ë°ì´í„° > ì¶©ë¶„í•œ ì€ë‹‰ ë…¸ë“œì˜ ìˆ˜ í•„ìš”

9-1. ì¸ì¡° ì‹ ê²½ë§ ëª¨í˜•(NN_type) ì‹¤ìŠµ

##### 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ë° Neural Network ì í•©

- í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

X = [[0., 0.], [1., 1.]]
y = [[0, 1], [1, 1]]

from sklearn.neural_network import MLPClassifier

- ëª¨ë¸ ì í•©

clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)
clf.fit(X,y)

clf.predict([[2.,2.], [-1.,-2.]])

clf.coefs_

[coef.shape for coef in clf.coefs_]

##### 2. modelì˜ ë³µì¡ë„ì— ë”°ë¥¸ í¼í¬ë¨¼ìŠ¤ ë¹„êµ

- ë¼ì´ë¸ŒëŸ¬ë¦¬ 

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier

- ì„¤ì •í•  parameterë“¤ì„ ì…ë ¥. hëŠ” ì‹œê°í™”ë¥¼ ì–¼ë§ˆë‚˜ ìì„¸í•˜ê²Œ í•  ê²ƒì¸ê°€ì— ëŒ€í•œ ìœ„í•œ ì„ì˜ì˜ ê°’.

h = .02
alphas = np.logspace(-5, 3, 5)
names = ['alpha ' + str(i) for i in alphas]

classifiers = []
for i in alphas:
    classifiers.append(MLPClassifier(solver='lbfgs', alpha=i, random_state=1,
                                     hidden_layer_sizes=[100, 100]))

- ë°ì´í„° ìƒì„±

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=0, n_clusters_per_class=1)

pd.DataFrame(X).head()
pd.DataFrame(y).head()

rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

- ì—¬ëŸ¬ ëª¨ì–‘ì˜ ì¶”ê°€ ë°ì´í„°ì…‹ ìƒì„±

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable]

figure = plt.figure(figsize=(17, 9))
i = 1

for X, y in datasets:
    # preprocess dataset, split into training and test part
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                   edgecolors='black', s=25)
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   alpha=0.6, edgecolors='black', s=25)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

figure.subplots_adjust(left=.02, right=.98)
plt.show()
