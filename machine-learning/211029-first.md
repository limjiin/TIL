Part.03 기본적인 머신 러닝 모형
1.  LDA (Linear Discriminant Analysis)
가정
각 숫자 집단은 정규 분포 형태의 확률분포를 가진다
각 숫자 집단은 비슷한 형태의 공분산 구조를 가진다.
LDA 결과 얻게 되는 decision boundary 특징
boundary에 직교하는 축 : 자료들을 이 축에 정사영 시킨 분포의 형태를 고려
평균의 차이를 극대화 하려면? 두 평균 vector의 차이 벡터를 이용
정사영 시킨 두 분표의 특징이 아래 둘을 동시에 달성하고자 함
평균의 차이는 최대화
두 분포의 각각 분산은 최소화
결국 분산 대비 평균의 차이를 극대화 하는 boundary를 찾고자 하는 것

2. LDA 수학적 개념 이해 : 다변량 정규분포
이변량 정규분포
정규분포
이변량 정규분포
다변량 정규분포 
로그 다변량 정규분포
다변량 정규분포의 LDA에의 적용
K번째 범주 자료의 분포함수
K번째 범주가 나타날 사전 확률을 고려하면
K번째, 1번째 범주에서 현재 자료가 나왔을 확률
마무리
LDA decision boundary : 평면과 이를 가로지르는 점선
공분산이 같다는 구조 아래 LDA가 0 ~ 1 사이에서 K인지 L인지 알 수 있다.
X의 이니 형이 들어가고 이것이 초평면 형태

3. LDA 모델 정의 및 추정
LDA 모델 정의 
LDA decision boundary
분산 대비 평균의 차이를 극대화 하는 boundary
가정
각 숫자 집단의 정규분포 형태의 확률 분포를 가진다
각 숫자 집단은 비슷한 형태의 공분산 구조를 가진다
확률분포 관점
Y가 K개의 범주를 가질 때
Y = K 일 때, 공분산 구조를 가지는 p개의 정규분포 변수의 분포
K와 관계 없는 공통 공분산 구조를 가짐으로 인해서 x에 대한 1차식(linear)으로 정리된다

4. LDA 수학적 개념 이해 : 사영 (Projection)
사영 (Projection)
목표 : 분산은 최소화하면서 평균을 최대화 하는 사영을 찾는 것
사영된 곳에서 평균의 차이와 분산을 표현해야 함
총정리
아이디어, 분산을 최소화 평균의 차이를 최대화하는 축에 수직인 boundary를 찾고자 함
투영을 통해 찾아낸 새로운 축 a = eigen vector
확률 모델로 찾아낸 decision boundary
장점
나이브 베이즈 모델과 달리, 설명변수 간의 공분산 구조를 반영
가정이 위반되더라도 비교적 robust
단점
가장 작은 그룹의 샘플 수가 설명 변수의 개수보다 많아야 함
정규분포 가정에 크게 벗어나는 경우 잘 설명하지 못함
y 범주 사이에 공분산 구조가 다른 경우를 반영하지 못함

5. LDA 심화적 이해
QDA 정의 및 이해
K와 관계 없는 공통 공분산 구조에 대한 가정을 버린 것이 QDA
y의 범주별로 서로 다른 공분산 구조를 가진 경우에 활용 가능
장점
y범주별 공분산 구조를 다르게 할 수 있음
단점
셜명변수의 개수가 많을 경우, 추정해야 하는 모수가 많아짐
샘플이 많이 필요

5-1. LDA (Linear Discriminant Analysis) 실습

##### 1. Linear Discriminant Analysis

- LDA 를 위한 함수 불러오기
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([1, 1, 1, 2, 2, 2])

- LDA 모델 구축

clf = LinearDiscriminantAnalysis()
clf.fit(X, y)

print(clf.predict([[-0.8, -1]]))

##### 2. Quadratic Discriminant Analysis

- QDA를 위한 함수 불러오기

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

- QDA 모델 구축

clf2 = QuadraticDiscriminantAnalysis()
clf2.fit(X, y)

print(clf2.predict([[-0.8, -1]]))

- LDA, QDA 비교

from sklearn.metrics import confusion_matrix
y_pred=clf.predict(X)
confusion_matrix(y,y_pred)

y_pred2=clf2.predict(X)
confusion_matrix(y,y_pred2)

##### 3. LDA, QDA의 시각적 비교

from sklearn.datasets import make_moons, make_circles, make_classification
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

h=0.2
names = ["LDA", "QDA"]
classifiers = [
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable
            ]

figure = plt.figure(figsize=(27, 9))
i = 1
# iterate over datasets
for ds in datasets:
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
[O    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, m_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   alpha=0.6)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

figure.subplots_adjust(left=.02, right=.98)
plt.show()

6. SVM (Support Vector Machine)
배경
데이터의 분포가정이 힘들 때, 아래의 데이터를 잘 나누려면?
Boundary에 집중
문제점
정확히 구분되지 않는 경우가 존재한다면?
적당한 error를 허용하고, 이를 최소화하도록 boundary를 결정
종속 변수 데이터 형태에 따라 나뉨
범주형 변수 : Support Vector classifier
연속형 변수 : Support Vector regression
SVM, SVR의 핵심
Model cost에 영향을 끼칠 점과 끼치지 않을 점을 margin을 통해 구분
SVM : 마진 안에 포함되거나, 반대 방향으로 분류된 점 들
SVR : 마진 바깥에 위치한 점들
decision boundary/ rule
Lagrange multiplier : 라그랑주 승수
최적화 문제(최소화 또는 최대화하는 값)를 풀 때, 최대화하는 동시에 한정하고 싶은 경우
장점 vs LDA
데이터의 분포 가정이 힘든 경우, covariance 구조를 고려하는 것은 비효율적
Boundary 근처의 관측치만을 고려할 수 있음
예측의 정확도가 높음
단점
C를 결정해야 함
모형 구축에 시간이 오래 걸림
One-Class  SVM (Support Vector Machine)
종속변수 정보가 없는 자료를 요약하는 데 SVM 사용
자료를 모두 포함하는 원을 활용

6-1. SVM (Support vector machine) 실습

##### 1. 데이터 불러오기, 및 SVM 적합

- 함수 불러오기

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

- 모델 적합

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

C = 1
clf = svm.SVC(kernel = 'linear', C=C)
clf.fit(X, y)

from sklearn.metrics import confusion_matrix

y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

##### 2. kernel SVM 적합 및 비교

- LinearSVC

clf = svm.LinearSVC(C=C, max_iter=10000)
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

- radial basis function

clf = svm.SVC(kernel = 'rbf', gamma = 0.7, max_iter=10000)
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

- polynomial kernel

clf = svm.SVC(kernel = 'poly', degree=3, C=C, gamma='auto')
clf.fit(X,y)
y_pred = clf.predict(X)
confusion_matrix(y, y_pred)

##### 시각적 비교

- 함수 정의

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy


def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

- 데이터 불러오기

iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

- 모델정의 및 피팅

C = 1.0 #Regularization parameter
models = (svm.SVC(kernel='linear', C=C),
          svm.LinearSVC(C=C, max_iter=10000),
          svm.SVC(kernel='rbf', gamma=0.7, C=C),
          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))
models = (clf.fit(X, y) for clf in models)

titles = ('SVC with linear kernel',
          'LinearSVC (linear kernel)',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel')

fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy,
                  cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel('Sepal length')
    ax.set_ylabel('Sepal width')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)

plt.show()

7. 의사결정나무 배경
변수들로 기준을 만들고 이것을 통하여 샘플을 분류하고 분류된 집단의 성질을 통하여 추정하는 모형
장점 : 해석력이 높음, 직관적, 범용성
단점 : 높은 변동성, 샘플에 민감할 수 있음
의사결정나무 용어
Node - 분류의 기준이 되는 변수가 위치 이를 기준으로 샘플을 나눔
Parent Node - 하위 노드
Root Node - 상위 노드가 없는 가장 위의 노드
Leaf node (Tip) - 하위 노드가 없는 가장 아래 노트
Internal node - Leaf node가 아닌 노드
Edge - 샘플을 분류하는 조건이 위치하는 곳
Deepth - Root node에서 특정 노드까지 도달하기 위해 거쳐야 하는 Edge의 수
반응 변수에 따라
범주형 변수 : 분류 트리
연속형 변수 : 회귀 트리 

8. 의사결정나무 수학적 개념 이해
엔트로피 (Entropy)
직관적 정의 : 0 또는 1일 확률이 최소, 0.5일 확률이 최대가 되게 하는 함수
Information Gain
Information Gain = Entropy before - Entropy after
Decision Tree의 특정 node 이전과 이후의 Entropy 차이
Classification Tree
Tree 조건에 따라, X가 가질 수 있는 영역을 Block으로 나누는 개념
나누어진 영역 안에 속하는 샘플의 특성을 통하여 Y를 추정
Rm의 구성
각각의 독립변수에 대하여,
범주형 : 각 범주에 따라
연속형 : 여러 개의 영역으로 임의로 나눔
나누어둔 영역들에 대해, 아래 measure를 가장 좋은 값으로 만드는 변수와 기준을 선택
엔트로피
오분류율
Regression Tree
Tree 조건에 따라, X가 가질 수 있는 영역을 Block으로 나누는 개념
나누어진 영역 안에 속하는 샘플의 특성을 통하여 Y를 추정

8-1. 의사결정나무(Decision Tree) 실습

##### 1. 함수 익히기 및 모듈 불러오기

- 함수 익히기

from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[1, 1]])

- 모듈 불러오기

from sklearn.datasets import load_iris
from sklearn import tree
from os import system

system("pip install graphviz")

import graphviz

- 데이터 로드

iris = load_iris()

##### 2. 의사결정나무 구축 및 시각화

- 트리 구축

clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)

- 트리의 시각화

dot_data = tree.export_graphviz(clf,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph=graphviz.Source(dot_data)

- 엔트로피를 활용한 트리

clf2 = tree.DecisionTreeClassifier(criterion='entropy')
clf2 = clf.fit(iris.data, iris.target)

dot_data2 = tree.export_graphviz(clf2,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph2 = graphviz.Source(dot_data2)

- 프루닝

clf3 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)

clf3.fit(iris.data, iris.target)

dot_data3 = tree.export_graphviz(clf3,
                                out_file=None,
                             feature_names=iris.feature_names,
                            class_names=iris.target_names,
                              filled=True, rounded=True,
                              special_characters=True
                             )
graph3 = graphviz.Source(dot_data3)

- Confusion Matrix 구하기

from sklearn.metrics import confusion_matrix
confusion_matrix(iris.target,clf.predict(iris.data))

confusion_matrix(iris.target,clf2.predict(iris.data))

confusion_matrix(iris.target,clf3.predict(iris.data))

##### 3. Training - Test 구분 및 Confusion matrix 계산

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(iris.data,
                                                    iris.target,
                                                    stratify=iris.target,
                                                    random_state=1)

clf4 = tree.DecisionTreeClassifier(criterion='entropy')

clf4.fit(X_train, y_train)

confusion_matrix(y_test, clf4.predict(X_test))

##### 4. Decision regression tree

- 모듈 불러오기 및 데이터 생성

import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

- Regression tree 구축

regr1 = tree.DecisionTreeRegressor(max_depth=2)
regr2 = tree.DecisionTreeRegressor(max_depth=5)

regr1.fit(X, y)
regr2.fit(X, y)

X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
X_test

y_1 = regr1.predict(X_test)
y_1

y_2 = regr2.predict(X_test)
y_2

plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",
            c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",
         label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()

dot_data4 = tree.export_graphviz(regr2, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True)

graph4 = graphviz.Source(dot_data4)
graph4

dot_data5 = tree.export_graphviz(regr1, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True)

graph5 = graphviz.Source(dot_data5)

9. 신경망 모형
배경 : 인간의 뉴런
시냅스를 통하여 여러 뉴런으로 부터 자극을 전달 받음
이를 종합하여 다른 뉴런에게 자극을 전달
 뉴런의 구조를 모델링
하나의 뉴런 : perceptron
여러 layer를 통하여 시냅스를 표현
Perceptron
하나의 뉴런
입력 데이터 혹은 다른 레이어의 출력물을 받아 결과값을 내는 구조
input, weights, activation function(활성함수)로 구성
Multi Layer Perceptron
하나의 Hidden layer(은닉노드) : 4 perceptron
2개의 종속변수 : 2 perceptron
인조 뉴런의 구조
Activation function : 활성함수
연속, 비선형, 단조증가, bounded, 점근성의 특성
가장 기본적인 : step function, sigmoid function(미분이 쉬움. 딥러닝 이전에 많이 사용함)
필요성 : 은닉 layer을 의미 있게 쌓아주는 역할
선형의 layer만 쌓인다면 결국 하나의 선형식이 됨
출력값의 range를 결정
신경망 모형의 구조 이해
input layer(입력층) : 입력 데이터를 의미
hidden layer : 입력값 : 입력 데이터 혹은 또 다른 hidden layer의 출력값 : 위의 입력 값을 받는 perceptron들의 집합
output layer : 입력값 : 마지막 hidden layer의 출력값 : 최종 결과물을 만들어내는 perceptron들의 집합
수학적 개념 이해 : 바이너리 논리연산과 perceptron
바이너리 논리 연산 : AND, OR, XOR 연산이 존재
단층 perceptron (single-layer perceptron)
속성이 2개인 경우, 평면상의 직선으로 표현 가능
perceptron의 classification
인조 뉴런 OR 연산 구축 - 알고리즘의 이해
Weight 업데이트 알고리즘
LR : weight를 변화시키는 정도
값이 너무 크면 정확한 해를 찾기 힘듦
값이 너무 작으면 수렴하기까지 시간이 오래 걸림
E : 정의된 error 값. 주로 실제 값과 예측값의 차이를 사용
Backpropagation (역전파 알고리즘)
Multi layer perceptron을 학습시키기 위한 방법
Output layer에서의 error의 변화량을 앞선 layer들로 전파한다는 개념
미분을 통해 접근
신경망 모형의 한계점
gradient vanishing
sigmoid 함수의 한계점
중간해 멈춤 현상
최적해(Global minima)에 이르기 전에 중간해(Local minima)에서 멈추는 현상
과적합 문제
Training set에 과하게 최적화 되는 문제
일반화 되지 않음
신경망 모형의 해결책
ReLU
계산이 간단 : 학습 속도가 매우 빠름
0 보다 큰 경우 기울기 유지
0보다 작은 경우 기울기가 없는 문제점은 다른 ReLU 계열 함수 이용
Pre-training : 미리 traning 시켜서 local minima 문제를 해결. 올바른 초기값 선정에 도움
Drop out을 통한 과적합 문제 완화
Hidden layer의 node를 임의의 확률에 따라 남김 (0.5~1 사이의 확률을 권장)
계산 속도도 증가
신경망 모형의 심화적 이해
초기값 문제
Restricted Boltzmann Machine
Y를 사용하지 않고 X만을 사용하여 weigth를 학습
서로 인접한 층 사이에서만 학습하여 서로 예측하고, 예측한 값이 최소가 되는 weight를 찾음
이 값을 초기값으로 선정
과적합
weight decay
가중치가 최대한 작은 값을 가지도록 penalty 부여
Ridge regression과 같은 아이디어
Activation function의 선택
ReLU의 단점 보완 : ELU
은닉 층 개수, 은닉 노드의 개수, 의미
은닉 노드를 지나치게 많이 늘리면 과적합의 문제 발성
복잡한 문제를 푸는 경우 > 충분한 은닉 층의 개수 필요
다양한 입력 데이터 > 충분한 은닉 노드의 수 필요

9-1. 인조 신경망 모형(NN_type) 실습

##### 1. 데이터 불러오기, 및 Neural Network 적합

- 함수 불러오기

X = [[0., 0.], [1., 1.]]
y = [[0, 1], [1, 1]]

from sklearn.neural_network import MLPClassifier

- 모델 적합

clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)
clf.fit(X,y)

clf.predict([[2.,2.], [-1.,-2.]])

clf.coefs_

[coef.shape for coef in clf.coefs_]

##### 2. model의 복잡도에 따른 퍼포먼스 비교

- 라이브러리 

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier

- 설정할 parameter들을 입력. h는 시각화를 얼마나 자세하게 할 것인가에 대한 위한 임의의 값.

h = .02
alphas = np.logspace(-5, 3, 5)
names = ['alpha ' + str(i) for i in alphas]

classifiers = []
for i in alphas:
    classifiers.append(MLPClassifier(solver='lbfgs', alpha=i, random_state=1,
                                     hidden_layer_sizes=[100, 100]))

- 데이터 생성

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=0, n_clusters_per_class=1)

pd.DataFrame(X).head()
pd.DataFrame(y).head()

rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

- 여러 모양의 추가 데이터셋 생성

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable]

figure = plt.figure(figsize=(17, 9))
i = 1

for X, y in datasets:
    # preprocess dataset, split into training and test part
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
    # and testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot also the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                   edgecolors='black', s=25)
        # and testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   alpha=0.6, edgecolors='black', s=25)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

figure.subplots_adjust(left=.02, right=.98)
plt.show()
